# Building Micrograd

This repository contains a simple implementation of a scalar-valued autograd engine and a neural net library with a PyTorch-like API, following the tutorial provided in this [YouTube video](https://www.youtube.com/watch?v=VMj-3S1tku0&t=6602s).

Check out the github repo for micrograd:
https://github.com/karpathy/micrograd

The motivation behind this learning is to learn the entire playlist: https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&pp=iAQB (Neural Networks: Zero to Hero) to build upon the building blocks of Neural Network before moving up all the way to building my own transformer, GPT model.

This will be beneficial for my work in LLM, as I do not want to just be an expert in calling APIs, vector storages and langchain. Realised many interviews with PhD senior DS always test the architecture of the models like XGBoost.

## Contents
- micrograd.ipynb contains the tutorial itself with my own notes
- micrograd_exercises.ipynb contains an exercise by Andrej himself (TBD)
- requirements.txt (Actually just any pytorch version is needed and graphviz on local machine and a python env)

Happy learning :)